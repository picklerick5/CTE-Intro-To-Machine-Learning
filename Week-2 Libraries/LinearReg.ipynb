{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LinearReg",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshsingh22/CTE-Intro-To-Machine-Learning/blob/master/Week-2%20Libraries/LinearReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiIODUENwVWn",
        "colab_type": "text"
      },
      "source": [
        "**LINEAR REGRESSION WITH ONE VARIABLE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeHfSk4HgG8-",
        "colab_type": "code",
        "outputId": "04560de2-4f5f-43f5-9ff0-77f4da7c524e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# used for manipulating directory paths\n",
        "import os\n",
        "\n",
        "# Scientific and vector computation for python\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library\n",
        "from matplotlib import pyplot\n",
        "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
        "\n",
        "# tells matplotlib to embed plots within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "#Ignore this - its for making a copy of the text file from net\n",
        "! pip install wget\n",
        "import wget\n",
        "url=\"https://github.com/asmital/MLDL-CTE-2019/blob/master/ex1data1.txt\"\n",
        "filename = wget.download(url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb_Yu5UlwZAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read comma separated data\n",
        "#Load your data from exdata1 text file\n",
        "# Use this command : data = np.loadtxt..\n",
        "X, y = data[:, 0], data[:, 1]\n",
        "\n",
        "m = y.size  # number of training examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjFkZiKXlHs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a column of ones to X. The numpy function stack joins arrays along a given axis. \n",
        "# The first axis (axis=0) refers to rows (training examples) \n",
        "# and second axis (axis=1) refers to columns (features).\n",
        "\n",
        "# Assert raises an exception if the cell is run again without initializing X.\n",
        "#DO NOT EXECUTE MORE THAN ONCE\n",
        "assert X.shape == (97,), \"X already contains a stack ones.\"\n",
        "\n",
        "X = np.stack([np.ones(m), X], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtDbJk4we33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeCost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute cost for linear regression. Computes the cost of using theta as the\n",
        "    parameter for linear regression to fit the data points in X and y.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1), where m is the number of examples,\n",
        "        and n is the number of features. We assume a vector of one's already \n",
        "        appended to the features so we have n+1 columns.\n",
        "    \n",
        "    y : array_like\n",
        "        The values of the function at each data point. This is a vector of\n",
        "        shape (m, ).\n",
        "    \n",
        "    theta : array_like\n",
        "        The parameters for the regression function. This is a vector of \n",
        "        shape (n+1, ).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    J : float\n",
        "        The value of the regression cost function.\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Compute the cost of a particular choice of theta. \n",
        "    You should set J to the cost.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize some useful values\n",
        "    m = y.size  # number of training examples\n",
        "    \n",
        "    # You need to return the following variables correctly\n",
        "    J = 0.0\n",
        "    \n",
        "    # ====================== YOUR CODE HERE =====================\n",
        "\n",
        "    \n",
        "    # ===========================================================\n",
        "    return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPsfIlSog7IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "J1 = computeCost(X, y, theta=np.array([0.0, 0.0]))\n",
        "print('With theta = [0, 0] \\nCost computed = %.2f' % J1)\n",
        "print('Expected cost value (approximately) 32.07\\n')\n",
        "\n",
        "# further testing of the cost function\n",
        "J2 = computeCost(X, y, theta=np.array([-1, 2]))\n",
        "print('With theta = [-1, 2]\\nCost computed = %.2f' % J2)\n",
        "print('Expected cost value (approximately) 54.24\\n')\n",
        "\n",
        "limit1 = 1.2\n",
        "est_Error1 = abs(J1 - 32) + abs(J2 - 54)\n",
        "\n",
        "if(est_Error1 <= limit1) : \n",
        "    print(\"Great work! You may proceed.\")\n",
        "else :\n",
        "    print(\"You may want to check your implementation.\")\n",
        "    print(\"[Estimated error : %.2f]\" % est_Error1)\n",
        "    assert False, \"******* Results don't match. *******\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HU76rKMhHYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
        "    gradient steps with learning rate `alpha`.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The input dataset of shape (m x n+1).\n",
        "    \n",
        "    y : arra_like\n",
        "        Value at given features. A vector of shape (m, ).\n",
        "    \n",
        "    theta : array_like\n",
        "        Initial values for the linear regression parameters. \n",
        "        A vector of shape (n+1, ).\n",
        "    \n",
        "    alpha : float\n",
        "        The learning rate.\n",
        "    \n",
        "    num_iters : int\n",
        "        The number of iterations for gradient descent. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    theta : array_like\n",
        "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
        "    \n",
        "    J_history : list\n",
        "        A python list for the values of the cost function after each iteration.\n",
        "    \n",
        "    Instructions\n",
        "    ------------\n",
        "    Peform a single gradient step on the parameter vector theta.\n",
        "\n",
        "    While debugging, it can be useful to print out the values of \n",
        "    the cost function (computeCost) and gradient here.\n",
        "    \"\"\"\n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0]  # number of training examples\n",
        "    \n",
        "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
        "    # are passed by reference to functions\n",
        "    theta = theta.copy()\n",
        "    \n",
        "    J_history = [] # Use a python list to save cost in every iteration\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # ==================== YOUR CODE HERE =================================\n",
        "        \n",
        "        \n",
        "        # =====================================================================\n",
        "        \n",
        "        # save the cost J in every iteration\n",
        "        J_history.append(computeCost(X, y, theta))\n",
        "    \n",
        "    print(\"----------------------------------------------------\\n\")\n",
        "    return theta, J_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBPAl4hJhkWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize fitting parameters\n",
        "theta = np.zeros(2)\n",
        "\n",
        "# Model hyperparameters. \n",
        "# # ==================== YOUR CODE HERE =================================\n",
        "# Feel free to change these if they don't seem to working.\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 15\n",
        "\n",
        "# QUESTION : What is the largest 'alpha' and smallest 'iterations' you can get away with ?\n",
        "\n",
        "# =====================================================================\n",
        "\n",
        "theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)\n",
        "print('Theta found by gradient descent: {:.4f}, {:.4f}'.format(*theta))\n",
        "print('Expected theta values (approximately): [-3.6303, 1.1664]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0NrulWWhrO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validate:\n",
        "\n",
        "limit2 = 1\n",
        "est_Error2 = abs(theta[0] + 3.6303) + abs(theta[1] - 1.664)\n",
        "\n",
        "if(est_Error2 <= limit2) : \n",
        "    print(\"Great work! Proceed to the Regression.\")\n",
        "else :\n",
        "    print(\"You may want to check your implementation.\")\n",
        "    print(\"[Estimated error : %.2f]\" % est_Error2)\n",
        "    assert False, \"******* Results don't match. *******\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQBgi3m1hyrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the linear fit\n",
        "plotData(X[:, 1], y)\n",
        "pyplot.plot(X[:, 1], np.dot(X, [-3.6303, 1.1664]), '-', c='black', linewidth=5)\n",
        "pyplot.plot(X[:, 1], np.dot(X, theta), '--', c='green')\n",
        "pyplot.legend(['Training data', 'Optimal Value', 'Your Output']);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTyCAIBnh28W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict1 = np.dot([1, 3.5], theta)\n",
        "print('For population = 35,000, we predict a profit of {:.2f}\\n'.format(predict1*10000))\n",
        "\n",
        "predict2 = np.dot([1, 7], theta)\n",
        "print('For population = 70,000, we predict a profit of {:.2f}\\n'.format(predict2*10000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVB0Vvoh8r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMiyKTpWlF0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}